# Pipeline for computing an array of biophysical metrics for monomeric proteins

## Summary

This pipeline computes an array of biophysical metrics for an input set of monomeric proteins. The metrics include the ones included in the study [Rocklin et al., 2017, Science](http://science.sciencemag.org/content/357/6347/168), and much of the code was derived from that paper. There are also additional metrics that we added since then.

Note: There are many ways that this pipeline can be improved! Please feel free to make improvements that you come up with and push them to the repository. That could be anything from making better documentation, expanding the number of metrics compute, or fixing errors that cause the pipeline to crash.

## Author contributions

Gabe Rocklin wrote the original code from Rocklin et al. Hugh Haddox, Alex Ford, and Gabe Rocklin adapted and expanded it into its current form in this repository.

## Organization of code

* `scripts/`: is a directory with a series of `Python` scripts that encode the scoring pipeline.
* `test_scoring.ipynb`: a notebook that runs the pipeline on ten structures from Rocklin et al., and then checks to see that the results from the pipeline match the results of the publication.
* `environment.yml`: a file listing a number of dependecies that are installable via `Conda` (see below).

## Installing external dependencies

Carrying out the pipeline requires multiple external dependencies. Unfortunately, the full set of required external dependencies are only available on the Baker lab server at this time. This includes:
* `PyRosetta`, as installed using `Conda` (see below).
* a few of the custom `Python` scripts for carying out the pipeline (e.g., `scripts/make_fragments.py` and `scripts/np_aa_burial.py`) refer to other scripts and programs on the Baker lab server that have not yet been extracted.

Nearly all dependencies are encoded in the file called `environment.yml`. If you're working on the Baker lab server, these dependencies can all be installed using [`Conda`](https://conda.io/docs/index.html). To do so, first clone this repository. Then, in the root directory of the repository, execute the command:

    conda env create -f environment.yml

## How to run the pipeline

Running the pipeline involves two command-line arguments:

First, activate the `Conda` environment described above using the command:

    source activate {environment_name}

where `environment_name` is the name of the environment. This will give the pipeline access to many of the required external dependencies.

Second, use `jug` to execute the pipeline:

    jug execute --jugdir {jugdir} scripts/score_designs.py {path_to_directory_with_input_pdbs} {path_to_output_directory}

* Inputs for the above command:
  * `{jugdir}`: the path to a directory where `jug` will store a variety of files needed to execute and keep track of the run.
  * `{path_to_directory_with_input_pdbs}`: the path to a directory with all input PDBs. Currently, all input PDBs need to be in this single directory.
  * `{path_to_output_directory}`: the path to a directory where all output will be stored, including temporary directories and files used in the computation and the final scores file.

* Outputs of the above command:
  * `scores.csv`: a CSV file with all computational metrics for each design, where columns are different metrics and rows are different designs. This file will be written to the directory specified by the input variable called `{path_to_output_directory}`.
  * the pipeline also generates a large number of temporary directories and files for each input structure, which are automatically deleted if the pipeline runs to completion. However, if the pipeline fails in the middle of the run, some of these files may not get deleted. See the below section called `What to do if the pipeline fails for some of the designs`

## Example Jupyter notebook that runs the pipeline with a handful of test PDBs

The notebook called `test_scoring.ipynb` runs the pipeline on ten structures from Rocklin et al., and then checks to see that the results from the pipeline match the results of the publication.

## How to run the pipeline using `sbatch`

You can use `sbatch` to parallelize the jobs across multiple CPUs. For an example of a file that can be used as input for `sbatch`, see the file called `results/test_scoring/run.sbatch`, which is generated by `test_scoring.ipynb`. To submit the job to `slurm`, the notebook simply executes the command: `sbatch results/test_scoring/run.sbatch`.

## Checking the status of the run
You can also use `jug` to check the "status" of the job, i.e., how many jobs have and have not been completed:

    jug status --jugdir {jugdir} scripts/score_designs.py {path_to_folder_with_pdbs} {path_to_output_folder}

Unfortunately, this takes a while since it still needs to initialize all the Rosetta XML filters, which takes several minutes. I haven't figured out a way around this.

## What to do if the pipeline fails

Sometimes the pipeline fails in ways that are obvious, where the script raises a clear error (e.g., KeyError). In these cases, the code needs to be changed in order for it to work correctly. Of note: any temporary files and directories should be deleted before rerunning the pipeline (see below).

However, there are other times when the pipeline fails for reasons that are still mysterious to me. In these cases, I can delete temporary files and directories, rerun the pipeline immediately, and it works.

In each of the above cases, I suggest deleting all temporary files and directories before rerunning the pipeline. Below is a list of possible directories/files to check for:
  * there can be a variety of temporary files in the directory in which you executed the `jug` command from step 2 from above.
  * there can be `temp_*` directories in the output directory specified by `{path_to_output_folder}`.
  * `jug` will sometimes create `.lock` files in the directory `{jugdir}/locks/`. These should be deleted.

## Description of metrics

Most metrics are described in the supplemental material of [Rocklin, 2017, Science](http://science.sciencemag.org/content/357/6347/168). Below are descriptions of some of the new metrics:

* `avg_all_frags_per_site`: a string with comma-delimited entries that report the average fragment quality within a sliding window centered upon each site. Specifically, these values are reported for each site that sits at the center of a 9mer window in the protein. Values are ordered by sites, and there are no values for the first four or last four sites in the protein since these sites do not sit at the center of any 9mer windows. Fragment quality is quantified as the RMS between the design and 9mer fragment. The reported number of each site is the average RMS over all fragments in a given window.
* `avg_all_frags_in_H`, `avg_all_frags_in_E`, `avg_all_frags_in_L`: each of these metrics is a float that gives the average fragment quality of all 9mers centered on sites in a given secondary structure (H: helix, E: strand, L: loop). Specifically, each value is computed using by considering all sites within a given structure, and averaging the corresponding site-specific values given in `avg_all_frags_per_site`.

## Ways to improve pipeline

See the issues section of the repo. Feel free to raise an issue yourself!

## Missing filters:
* `cavity_volume`
    * Currently gives rise to the error:
        Error: Element 'CavityVolume': This element is not expected. Expected is one of ( AASynthesisFitnessCost, AlaScan, AlignmentAAFinder, AlignmentGapInserter, AngleToVector, AtomCount, AtomicContact, AtomicContactCount, AtomicDistance, AverageDegree ).
    * Parisa said that this may not yet be part of master
